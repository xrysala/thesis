{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Perform Topic Modeling using BERT","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_docs = []\nfor k in range(1, 123+1 ):\n    try:\n        data = pd.read_csv(f\"/kaggle/input/requirements/{k}.txt\", header=None, sep='\\t')\n        documents = data.to_numpy().ravel().tolist()\n        one_str  = ' '.join(documents)\n        all_docs.append(one_str)\n    except pd.errors.ParserError:\n        print(f\"Error reading file: {k}.txt\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'Doc': all_docs})  #--> Every document is a project","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install sentence_transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install umap-learn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install hdbscan","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = model.encode(all_docs, show_progress_bar=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import umap.umap_ as umap\numap_embeddings = umap.UMAP(n_neighbors=4, \n                            n_components=5, \n                            metric='cosine',\n                           random_state=123).fit_transform(embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import hdbscan\ncluster = hdbscan.HDBSCAN(min_cluster_size=3,\n                          metric='euclidean',                      \n                          cluster_selection_method='eom')\nhdbscan_labels = cluster.fit_predict(umap_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\numap_data = umap.UMAP(n_neighbors=4, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n\n# Get unique cluster labels and their corresponding colors\nunique_labels = np.unique(cluster.labels_)\nnum_clusters = len(unique_labels)\ncolors = plt.cm.rainbow(np.linspace(0, 1, num_clusters))\n\n# Plot scatter plot with different colors per cluster\nfig, ax = plt.subplots(figsize=(10, 10))\nfor label, color in zip(unique_labels, colors):\n    if label == -1:  # Outliers\n        cluster_points = umap_data[cluster.labels_ == label]\n        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], color='#BDBDBD', s=10, label='Outliers')\n    else:  # Clusters\n        cluster_points = umap_data[cluster.labels_ == label]\n        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], color=color, s=10, label=f'Cluster {label}')\n\nax.set_title('Clustering')\nax.legend()\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = cluster.labels_\n\n# Count the number of clusters (excluding noise points labeled as -1)\nnum_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint(\"Number of clusters:\", num_clusters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs_df = pd.DataFrame(all_docs, columns=[\"Doc\"])\ndocs_df['Topic'] = cluster.labels_\ndocs_df['Doc_ID'] = range(len(docs_df))\ndocs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs_with_topic = docs_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef c_tf_idf(documents, m, ngram_range=(1, 1)):\n    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n    t = count.transform(documents).toarray()\n    w = t.sum(axis=1)\n    tf = np.divide(t.T, w)\n    sum_t = t.sum(axis=0)\n    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n    tf_idf = np.multiply(tf, idf)\n\n    return tf_idf, count\n  \ntf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(all_docs))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n    words = count.get_feature_names_out()\n    labels = list(docs_per_topic.Topic)\n    tf_idf_transposed = tf_idf.T\n    indices = tf_idf_transposed.argsort()[:, -n:]\n    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n    return top_n_words\n\ndef extract_topic_sizes(df):\n    topic_sizes = (df.groupby(['Topic'])\n                     .Doc\n                     .count()\n                     .reset_index()\n                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n                     .sort_values(\"Size\", ascending=False))\n    return topic_sizes\n\ntop_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\ntopic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Temporarily suppress warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n\n    original_top_n_words = pd.DataFrame(columns=['Key', 'New'])\n    for key, value in top_n_words.items():\n        new = [sublist[0] for sublist in value]\n        original_top_n_words = original_top_n_words.append({'Key': key, 'New': new}, ignore_index=True)\n\n    print(original_top_n_words) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None) # Run this if you want to display the whole content\n# pd.set_option('display.max_colwidth', 50) # Run this to return to default display options","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = {\n    'Topic': [],\n    'Top Words': [],\n    'Size': []\n}\n\n# Iterate over the object\nfor topic, tuples in top_n_words.items():\n    words = [word for word, _ in tuples]\n    data['Topic'].append(topic)\n    data['Top Words'].append(', '.join(words))\n    data['Size'].append(int(topic_sizes[topic_sizes['Topic'] == topic]['Size']))\n\n\n# Create a dataframe from the data dictionary\npd.DataFrame(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## A loop over n_neighbors, n_components and min_cluster_size values to find the right combination\nimport random\n\nn_neighbors = 20\nn_components = 2\nmin_cluster_size = 4\n\nwhile True:\n    umap_embeddings = umap.UMAP(n_neighbors=n_neighbors, \n                                n_components=n_components, \n                                metric='cosine',\n                                random_state=123).fit_transform(embeddings)\n\n    cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n                              metric='euclidean',                      \n                              cluster_selection_method='eom').fit(umap_embeddings)\n    \n    cluster_sizes = pd.Series(cluster.labels_).value_counts()\n    largest_cluster_size = cluster_sizes.max()\n    \n    if largest_cluster_size <= 80:\n        break\n    labels = cluster.labels_\n\n    # Count the number of clusters (excluding noise points labeled as -1)\n#     num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n#     if num_clusters <= 15 and num_clusters >= 8:\n#         print('Found!')\n#         break\n    \n    # Update parameter values \n    n_neighbors = random.randint(2, 10)\n    n_components = random.randint(2, 10)\n    min_cluster_size = random.randint(2, 10)\n\nprint(\"Final values:\")\nprint(\"n_neighbors:\", n_neighbors)\nprint(\"n_components:\", n_components)\nprint(\"min_cluster_size:\", min_cluster_size)\nprint(\"num_clusters:\", num_clusters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################################################################################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## internal topic modeling","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"big_df = pd.DataFrame()\nsub_cluster_length = 0\nfor i in range(len(docs_per_topic)):\n    topic_id = docs_per_topic.iloc[i][0]\n    topic = re.split(r'(?<=[.!?])\\s+', docs_per_topic.iloc[i][1])\n    internal_embeddings = model.encode(topic, show_progress_bar=True)\n    internal_umap_embeddings = umap.UMAP(n_neighbors=4, n_components=5, min_dist=0.0, metric='cosine').fit_transform(internal_embeddings)\n\n    if len(topic) >= 300: # > 300\n        internal_umap_embeddings = umap.UMAP(n_neighbors=4, n_components = 5, min_dist=0.0, metric='cosine').fit_transform(internal_embeddings)\n        internal_cluster = hdbscan.HDBSCAN(min_cluster_size=10,\n                                        metric='euclidean',                      \n                                        cluster_selection_method='eom').fit(internal_umap_embeddings)\n    else: # < 300\n        internal_umap_embeddings = umap.UMAP(n_neighbors=4, n_components = 2, min_dist=0.0, metric='cosine').fit_transform(internal_embeddings)\n        internal_cluster = hdbscan.HDBSCAN(min_cluster_size=2,\n                                        metric='euclidean',                      \n                                        cluster_selection_method='eom').fit(internal_umap_embeddings)\n#     result = pd.DataFrame(internal_umap_embeddings, columns=['x', 'y'])\n#     result['labels'] = internal_cluster.labels_\n    docs_df = pd.DataFrame(topic, columns=[\"Doc\"])\n    docs_df['Small_Cluster'] = internal_cluster.labels_\n    docs_df['Doc_ID'] = range(len(docs_df))\n    docs_per_topic_pertopic = docs_df.groupby(['Small_Cluster'], as_index = False).agg({'Doc': ' '.join})\n    docs_per_topic_pertopic['Big_Cluster'] = topic_id\n    big_df = big_df.append(docs_per_topic_pertopic, ignore_index = True)\n    \n    sub_cluster_length = sub_cluster_length + len(np.unique(internal_cluster.labels_))\n    \nprint(\"Mean sub-cluster length:\", sub_cluster_length / i)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the order of the columns\ncols = big_df.columns.tolist()\n\ncols = cols[-1:] + cols[:-1]\n\nall_clusters_df = big_df[cols]  #    OR    big_df = big_df.ix[:, cols]\n\nall_clusters_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_sentence = \"\"\"The system must be able to record a playing movie.\nThe system must be able to return to the menu when a movie is playing.\nThe system must be able to provide an option menu for a selected movie.\nThe system must be able to provide the ability to select subtitles for a selected movie.\nThe system must be able to provide a list of TV channels.\nThe user must be able to select a TV channel.\nThe system must be able to provide a list of program categories for a selected TV channel.\nThe system must be able to provide the weekly program for a selected channel.\nThe system must be able to project a selected program.\nThe system must be able to pause and stop a playing program.\nThe system must be able to forward and rewind a playing program.\nThe system must be able to record a playing program.\n\"\"\"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef compute_tf_idf(new_sentence, existing_documents):\n    documents = existing_documents + [new_sentence]\n\n    vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n    tfidf_matrix = vectorizer.fit_transform(documents)\n\n    new_sentence_tfidf = tfidf_matrix[-1]  # Extract TF-IDF vector for the new sentence\n\n    return new_sentence_tfidf, vectorizer\n\nnew_sentence_tfidf, vectorizer = compute_tf_idf(new_sentence, all_docs)\n\nfeature_names = vectorizer.get_feature_names_out()\ntop_n_words_indices = np.argsort(new_sentence_tfidf.toarray())[0, -10:]  # Get indices of top 10 words\ntop_n_words = [(feature_names[i], new_sentence_tfidf[0, i]) for i in top_n_words_indices[::-1]]\n\nprint(\"Top words in the new sentence:\")\nfor word, tfidf_score in top_n_words:\n    print(f\"{word}: {tfidf_score}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_list = [sublist[0] for sublist in top_n_words]\nnew_list = []\nfor sublist in top_n_words:\n    new_list.append(sublist[0])\n\nprint(new_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The clusters: \")\nprint(original_top_n_words)\nprint(\"The top words of the new input:\",new_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Initialize variables to track the most similar item\nmost_similar_item = None\nhighest_similarity_score = -1\n\n# Iterate through each item in the DataFrame\nfor index, row in original_top_n_words.iterrows():\n    new_new_list = row['New']\n    \n    # Convert the given list and the current item to sentence embeddings\n    given_list_embedding = model.encode(\" \".join(new_list), convert_to_tensor=True)\n    new_list_embedding = model.encode(\" \".join(new_new_list), convert_to_tensor=True)\n    \n    \n    # Reshape the embeddings to match the input format for cosine_similarity\n    given_list_embedding = given_list_embedding.reshape(1, -1)\n    new_list_embedding = new_list_embedding.reshape(1, -1)\n    \n    # Compute the cosine similarity between the embeddings\n    similarity_score = cosine_similarity(given_list_embedding, new_list_embedding)[0][0]\n    \n    # Check if the current item has a higher similarity score\n    if similarity_score > highest_similarity_score:\n        highest_similarity_score = similarity_score\n        most_similar_item = row['New']\n        assigned_cluster_id = row['Key']\n\nprint(\"Most similar item:\", most_similar_item)\nprint(\"Assigned Cluster id:\", assigned_cluster_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrieve projects with the same topic \nfiltered_df = docs_with_topic[docs_with_topic['Topic'] == int(assigned_cluster_id)]\n# Retrieve the `Doc_ID` values where `Topic` is equal to assigned_cluster_id\ndoc_ids = filtered_df['Doc_ID'].tolist()\nprint(\"Similar projects are the projects with ids:\", doc_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}